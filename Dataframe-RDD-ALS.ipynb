{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext,Row,SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS as ML_ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from time import time\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.mllib.recommendation import Rating,ALS as MLLIB_ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-0a58089c11ed>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0a58089c11ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"G:\\spark-2.2.0-bin-hadoop2.7\\python\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mG:\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32mG:\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-0a58089c11ed>:7 "
     ]
    }
   ],
   "source": [
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"G:\\spark-2.2.0-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"G:\\spark-2.2.0-bin-hadoop2.7\\python\")\n",
    "\n",
    "sc = SparkContext().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataFrame_with_out_RDD():\n",
    "    def __init__(self, sql_context, movies_csv, ratings_csv):\n",
    "        self.movies_csv = movies_csv\n",
    "        self.ratings_csv = ratings_csv\n",
    "        self.sql_context = sql_context\n",
    "        self.als = ML_ALS()\n",
    "\n",
    "    \n",
    "    def get_spark_df(self, csv_file, schema):\n",
    "        t0 = time()\n",
    "        spark_df = self.sql_context.read.format('csv') \\\n",
    "            .option('delimeter', '\\t') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(csv_file, schema=StructType(schema))\n",
    "        tt = time() - t0\n",
    "        #print(\"*************************Time Consumed during creating {0} {1} *******************\".format(schema,round(tt,3)))        \n",
    "        return spark_df\n",
    "    \n",
    "    def get_movies_schema(self):\n",
    "        movies_schema = [\n",
    "            StructField(\"movieId\", IntegerType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"genres\", StringType(), True)\n",
    "        ]\n",
    "        return movies_schema\n",
    "\n",
    "    def get_ratings_schema(self):\n",
    "        ratings_schema = [\n",
    "            StructField(\"userID\", IntegerType(), True),\n",
    "            StructField(\"movieID\", IntegerType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "        ]\n",
    "        return ratings_schema\n",
    "    \n",
    "    def get_best_rank(self,ranks,errors,models,count,min_error,reg_val,training_df,validation_df,test_df):\n",
    "        best_rank = -1\n",
    "        for rank in ranks:\n",
    "            als = self.als.setRank(rank)\n",
    "            model = als.fit(training_df)\n",
    "\n",
    "            #make the prediction on the validation set\n",
    "            predict_df = model.transform(validation_df)\n",
    "            predicted_rating_df = predict_df.filter(predict_df.prediction !=float('nan'))\n",
    "            predicted_rating_df.cache()\n",
    "            \n",
    "            #Run the previously created RMSE evaluator,reg_val,on the predicted_rating_df\n",
    "            error = reg_val.evaluate(predicted_rating_df)\n",
    "            errors[count] = error\n",
    "            models[count] = model\n",
    "            print('For rank %s the RMSE is %s'% (rank,error))\n",
    "\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = count\n",
    "            count +=1\n",
    "        return best_rank\n",
    "    \n",
    "    def build_model(self):\n",
    "        t0 = time()\n",
    "        \n",
    "        movies_schema = DataFrame_with_out_RDD.get_movies_schema(self)\n",
    "        raw_movies_df = DataFrame_with_out_RDD.get_spark_df(self, self.movies_csv, movies_schema)\n",
    "        # print movies data\n",
    "        '''\n",
    "        print(\"Movies Data: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        raw_movies_df.show()\n",
    "        '''\n",
    "        \n",
    "        # removed genres from movies_df\n",
    "        movies_df = raw_movies_df.drop('genres')\n",
    "\n",
    "        '''\n",
    "        print(\"After removed genres from movies dataframe !!!!!!!!!!!!!\")\n",
    "        movies_df.show()\n",
    "        '''\n",
    "        \n",
    "        ratings_schema = DataFrame_with_out_RDD.get_ratings_schema(self)\n",
    "        ratings_df = DataFrame_with_out_RDD.get_spark_df(self, self.ratings_csv, ratings_schema)\n",
    "        \n",
    "        print(\"Rating Data: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        ratings_df.show()\n",
    "        \n",
    "        \n",
    "        # cached both DataFrames\n",
    "        movies_df.cache()\n",
    "        ratings_df.cache()\n",
    "\n",
    "        # Splitting the ratings DataFrame\n",
    "        (training_df, validation_df, test_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "        # Now cached the data splits\n",
    "        training_df.cache()\n",
    "        validation_df.cache()\n",
    "        test_df.cache()\n",
    "\n",
    "        # initailze the ALS() method from ml.recommendations\n",
    "        print(\"Initialize the ALS \")\n",
    "        als =self.als\n",
    "        MAX_ITERATIONS  = 10\n",
    "        REG_PARAM = 0.01\n",
    "        SEED_VALUE = 42\n",
    "\n",
    "        als.setMaxIter(MAX_ITERATIONS)\\\n",
    "            .setSeed(SEED_VALUE)\\\n",
    "            .setRegParam(REG_PARAM)\\\n",
    "            .setUserCol('userID')\\\n",
    "            .setItemCol('movieID')\\\n",
    "            .setRatingCol('rating')\\\n",
    "           \n",
    "        ranks=[1,3,5,7]\n",
    "        errors = [0,0,0,0]\n",
    "        models = [0,0,0,0]\n",
    "        count = 0\n",
    "        min_error = float('inf') #greater than any other number\n",
    "        \n",
    "        #Create RMSE evaluator using the label and prediction columns\n",
    "        reg_val = RegressionEvaluator(predictionCol='prediction',labelCol='rating',metricName='rmse')\n",
    "\n",
    "        best_rank = DataFrame_with_out_RDD.get_best_rank(self,ranks,errors,models,count,min_error,reg_val,training_df,validation_df,test_df)\n",
    "        \n",
    "        als.setRank(ranks[best_rank])\n",
    "        print('The best model was trained with rank {0}'.format(ranks[best_rank]))\n",
    "        \n",
    "        #Select the best model \n",
    "        b_model = models[best_rank]\n",
    "        \n",
    "        #Apply model on test dataset \n",
    "        test_predict_df = b_model.transform(test_df)\n",
    "        test_predict_df.cache()\n",
    "        \n",
    "        test_predict_df = test_predict_df.filter(test_predict_df.prediction != float('NaN'))\n",
    "        test_predict_df.cache()\n",
    "        \n",
    "        #Compute the test accuracy \n",
    "        test_RMSE = reg_val.evaluate(test_predict_df)\n",
    "        \n",
    "        print('The model had a RMSE on the test set of {0}'.format(test_RMSE))        \n",
    "        tt = time() - t0\n",
    "        print(\"Model build in %s seconds\"%round(tt,3))\n",
    "        print(\"Representation of DataFrame: \")\n",
    "        print(test_predict_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrame_with_RDD():\n",
    "    def __init__(self,SC,rating_csv):\n",
    "        self.SC = SC\n",
    "        self.rating_csv = rating_csv\n",
    "        \n",
    "    def get_values(self,ranks,models,min_error,MSErrors,ratings,MAX_ITERATIONS,SEED_VALUE):\n",
    "        count = 0\n",
    "        best_rank = -1\n",
    "        for rank in ranks:\n",
    "            model = MLLIB_ALS.train(ratings=ratings, rank=rank, iterations=MAX_ITERATIONS, seed=SEED_VALUE)\n",
    "            test_data = ratings.map(lambda p: (p[0], p[1]))\n",
    "            prediction = model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            ratesAndPred = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(prediction)\n",
    "            MSError = ratesAndPred.map(lambda r: (r[1][0] - r[1][1]) ** 2).mean()\n",
    "            print(\"Error of %s = %s \" % (rank, MSError))\n",
    "            MSErrors[count] = MSError\n",
    "            models[count] = model\n",
    "            if MSError < min_error:\n",
    "                min_error = MSError\n",
    "                best_rank = count\n",
    "            count += 1\n",
    "        return best_rank,test_data,prediction,ratesAndPred\n",
    "    \n",
    "    def get_ratings_rdd(self):\n",
    "        t0 = time()\n",
    "        ratings_rdd = self.SC.textFile(self.rating_csv)\n",
    "        header = ratings_rdd.first()\n",
    "        ratings_rdd = ratings_rdd.filter(lambda row: row != header) \n",
    "        ratings = ratings_rdd.map(lambda l: l.split(','))\\\n",
    "                .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "        time_taken = time() - t0\n",
    "        #print(\"*************************Time Consumed during creating rdd {0} *******************\".format(round(time_taken,3)))\n",
    "        print(ratings.take(10))\n",
    "        return ratings\n",
    "        \n",
    "    def build_model(self):\n",
    "        t0 = time()\n",
    "        ratings = DataFrame_with_RDD.get_ratings_rdd(self)\n",
    "        ranks = [3, 5, 7]\n",
    "        Errors = [0, 0, 0, 0]\n",
    "        models = [0, 0, 0, 0]\n",
    "        count = 0\n",
    "        min_error = float('inf')  # greater than any other number\n",
    "        MAX_ITERATIONS = 10\n",
    "        SEED_VALUE = 42\n",
    "        best_rank,test_data,prediction,rateAndPred = DataFrame_with_RDD.get_values(self,ranks,models,min_error,\\\n",
    "                                                                    Errors,ratings,MAX_ITERATIONS,SEED_VALUE)\n",
    "        print('************* The best model was trained with rank {0} *******************'.format(ranks[best_rank],Errors[best_rank]))\n",
    "        time_taken = time() - t0\n",
    "        print(\"****************** Time Consumed during model creation {0} *******************\".format(round(time_taken,3)))\n",
    "        print(\"Representation of RDD: \")\n",
    "        print(rateAndPred.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe !!!!!!!!!!!!!!!!!!\n",
      "Rating Data: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "+------+-------+------+\n",
      "|userID|movieID|rating|\n",
      "+------+-------+------+\n",
      "|     1|     31|   2.5|\n",
      "|     1|   1029|   3.0|\n",
      "|     1|   1061|   3.0|\n",
      "|     1|   1129|   2.0|\n",
      "|     1|   1172|   4.0|\n",
      "|     1|   1263|   2.0|\n",
      "|     1|   1287|   2.0|\n",
      "|     1|   1293|   2.0|\n",
      "|     1|   1339|   3.5|\n",
      "|     1|   1343|   2.0|\n",
      "|     1|   1371|   2.5|\n",
      "|     1|   1405|   1.0|\n",
      "|     1|   1953|   4.0|\n",
      "|     1|   2105|   4.0|\n",
      "|     1|   2150|   3.0|\n",
      "|     1|   2193|   2.0|\n",
      "|     1|   2294|   2.0|\n",
      "|     1|   2455|   2.5|\n",
      "|     1|   2968|   1.0|\n",
      "|     1|   3671|   3.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Initialize the ALS \n",
      "For rank 1 the RMSE is 1.3311724107946843\n",
      "For rank 3 the RMSE is 1.0384365334796244\n",
      "For rank 5 the RMSE is 1.1228417078768396\n",
      "For rank 7 the RMSE is 1.1756376244225129\n",
      "The best model was trained with rank 3\n",
      "The model had a RMSE on the test set of 1.0397066285097418\n",
      "Model build in 405.602 seconds\n",
      "Representation of DataFrame: \n",
      "+------+-------+------+----------+\n",
      "|userID|movieID|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|   350|    471|   3.0|  4.137455|\n",
      "|   440|    471|   3.0| 3.0932813|\n",
      "|   306|    471|   3.0|   4.05369|\n",
      "|   299|    471|   4.5|  3.902524|\n",
      "|   354|    471|   5.0|   3.66808|\n",
      "|   105|    471|   4.0| 3.6642911|\n",
      "|   529|    471|   4.0|  3.273617|\n",
      "|   184|    471|   5.0| 3.3575113|\n",
      "|   468|    471|   4.0| 2.8446436|\n",
      "|   497|    496|   2.0| 2.3953738|\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    sql_context = SQLContext(sc)\n",
    "    print(\"Dataframe !!!!!!!!!!!!!!!!!!\")\n",
    "    movie_csv_file = \"C:\\\\Users\\\\Rohit Singh\\\\Desktop\\\\Internship Task\\\\ml-latest-small\\\\movies.csv\"\n",
    "    rating_csv_file = \"C:\\\\Users\\\\Rohit Singh\\\\Desktop\\\\Internship Task\\\\ml-latest-small\\\\ratings.csv\"\n",
    "\n",
    "    ALS_with_Dataframe =  DataFrame_with_out_RDD(sql_context,movie_csv_file,rating_csv_file)\n",
    "    ALS_with_Dataframe.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD !!!!!!!!!!\n",
      "[Rating(user=1, product=31, rating=2.5), Rating(user=1, product=1029, rating=3.0), Rating(user=1, product=1061, rating=3.0), Rating(user=1, product=1129, rating=2.0), Rating(user=1, product=1172, rating=4.0), Rating(user=1, product=1263, rating=2.0), Rating(user=1, product=1287, rating=2.0), Rating(user=1, product=1293, rating=2.0), Rating(user=1, product=1339, rating=3.5), Rating(user=1, product=1343, rating=2.0)]\n",
      "Error of 3 = 0.511299294590665 \n",
      "Error of 5 = 0.407893721305383 \n",
      "Error of 7 = 0.3477487383536872 \n",
      "************* The best model was trained with rank 7 *******************\n",
      "****************** Time Consumed during model creation 106.683 *******************\n",
      "Representation of RDD: \n",
      "[((1, 31), (2.5, 2.416029994638364)), ((1, 1129), (2.0, 2.4972826898952007)), ((1, 2193), (2.0, 1.8273437104380568)), ((2, 110), (4.0, 4.175767829222066)), ((2, 144), (3.0, 2.866384563264269)), ((2, 266), (5.0, 4.072009131080138)), ((2, 292), (3.0, 3.458009204354747)), ((2, 364), (3.0, 4.012504129337801)), ((2, 370), (2.0, 2.39066636423311)), ((2, 372), (3.0, 3.5415858730439354))]\n"
     ]
    }
   ],
   "source": [
    "    print(\"RDD !!!!!!!!!!\")\n",
    "    SC = sc\n",
    "    ALS_with_RDD =  DataFrame_with_RDD(SC,rating_csv_file)\n",
    "    ALS_with_RDD.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comaprison between DatFrame and RDD Using PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Time Consumed while using  ALS() for recommendation:\n",
    "         Dataframe take more time as compare to Rdd while using ALS().\n",
    "\n",
    "##  ml use by  Dataframe : \n",
    "        from pyspark.ml.recommendation import ALS as ML_ALS\n",
    "\n",
    "##  mllib use by RDD:\n",
    "        from pyspark.mllib.recommendation import Rating,ALS as MLLIB_ALS \n",
    "\n",
    "##  RDD: \n",
    "        RDD is stand for Resilent Distributed Dataset.\n",
    "        RDD use collection and better for unstructured data.\n",
    "        RDD is good when we dont want impose schema such as columns format\n",
    "        RDD can be converted into Dataframe \n",
    "\n",
    "##  Dataframe: \n",
    "        Dataframe is also a distributed collection of data.\n",
    "        IN DataFrame data is organised into named column like a relational table.\n",
    "        It is designed for large dataset processing.APIs of Dataframe are easy to use \n",
    "        similar to SQL language.\n",
    "        \n",
    "# Union of RDD and Dataframe is Datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
